import numpy as np
import os
import torch

from torch.utils.data import Dataset
from hypothesis.io.simulation import inputs_path
from hypothesis.io.simulation import load_block
from hypothesis.io.simulation import num_blocks
from hypothesis.io.simulation import outputs_path
from hypothesis.io.simulation import sanitize_path



class GeneratorDataset(Dataset):
    r"""
    A dataset sampling directly from a generative model or simulator
    under a given prior.

    Arguments:
        model (Module): Generative model to sample from
        prior (Distribution): Probability distribution over the inputs
        size (int): Assumed number of samples in the dataset.

    .. note::
        When iterating over multiple epochs, this dataset will have
        different contents as we will sample from the prior in
        an online fashion. Furthermore, the potential stochasticity
        of the model also contributes to this fact.
    """

    def __init__(self, model, prior, size=100000):
        super(GeneratorDataset, self).__init__()
        self.prior = prior
        self.model = model
        self.size = size

    def _sample(self):
        x = self.prior.sample()
        y = self.model(x)

        return x, y

    def __getitem__(self, index):
        return self._sample()

    def __len__(self):
        return self.size



class SimulationDataset(Dataset):
    r"""A dataset that has been generated by a simulator.

    This dataset accepts a path to a simulated dataset generated by `simulation_block_write`. In
    essence, the dataset is seperated into two main partittions, 'thetas' (simulation parameters), and
    `x` (simulation outputs). In each individual part, these are stored in compressed 'blocks' of arbitrary
    size. The hyperparameters of the dataset (e.g., block-size) will be automatically determined by this class.

    Arguments:
        path (str): The path to the root folder of the dataset

    .. note::
        Blocks are fully filled, no block is partially filled.
    """

    def __init__(self, path):
        super(SimulationDataset, self).__init__()
        # Basic dataset parameters
        path = sanitize_path(path)
        self.base = path
        self.base_inputs = inputs_path(path)
        self.base_outputs = outputs_path(path)
        # Buffering
        self.buffer_block_index = 0
        self.buffer_inputs, self.buffer_outputs = self._load_block(self.buffer_block_index)
        # Block parameters
        self.num_blocks = self._num_blocks()
        self.block_elements = len(self.buffer_inputs)
        # Dataset size
        self.size = self.block_elements * self.num_blocks

    def _load_block(self, block_index):
        return load_block(self.path, block_index)

    def _num_blocks(self):
        return num_blocks(self.path)

    def __getitem__(self, index):
        # Check if the block is buffered in memory.
        block_index = int(index / self.block_elements)
        if block_index != self.buffer_block_index:
            inputs, outputs = self._load_block(block_index)
            self.buffer_inputs = inputs
            self.buffer_outputs = outputs
            self.buffer_block_index = block_index
        # Load the requested data from the buffer.
        data_index = index % self.block_elements
        inputs = self.buffer_inputs[data_index]
        outputs = self.buffer_outputs[data_index]

        return inputs, outputs

    def __len__(self):
        return self.size
